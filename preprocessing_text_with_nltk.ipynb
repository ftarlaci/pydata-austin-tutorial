{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and packages that we discussed in the introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "import pandas as pd\n",
    "#TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work with the provided sentence below and load it into a dataframe using Pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process the sentences below that could be interpreted as positive, negative\n",
    "dataset = ['I am enjoying my time at PyData Austin. Austin is a beautiful city but I don\\'t like the traffic!']   \n",
    "\n",
    "# TODO: Your code here\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting data. \n",
    "\n",
    "+ Some of the nltk built-in methods that we could use to prepare data for inspecting include: <br>\n",
    "\n",
    "    + `nltk.tokenizer.word_tokenize()`<br>\n",
    "    + `nltk.PorterStemmer` or `SnowballStemmer` class to reduce words into their stems<br>\n",
    "    + `nltk.tokenize.WhitespaceTokenizer()`  extracts tokens from string of words or sentences without whitespaces, new line, or tabs <br>\n",
    " \n",
    "+ See the examples below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming Example \n",
    "# TODO: Run this cell to see the output\n",
    "\n",
    "# Create an instance of the PorterStemmer()\n",
    "stemmer = PorterStemmer()  \n",
    "\n",
    "# Create a sentence and apply stemming\n",
    "sentence = \"Sentiment analysis is one of the subfields of Natural Language Processing\"\n",
    "   \n",
    "# Note: A word stem need not be the same root as a dictionary-based morphological root, \n",
    "# it just is an equal to or smaller form of the word. \n",
    "for w in words: \n",
    "    print(w, \" : \", stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer example\n",
    "# TODO: Run this cell to see the output\n",
    "\n",
    "sentence = \"Sentiment analysis is one of the subfields of Natural Language Processing\"\n",
    "words = word_tokenize(sentence) \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your own example and apply tokenize() and PorterStemmer(), inspect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `nltk` library offers many tokenizers that tokenize text in more nuanced ways. `WhitespaceTokenizer()` is one of them. These specific tokenizers allow further refining and fine-tuning of the data. Such more refined tokenizers might sometimes significantly contribute to the results that you gain from your NLP models depending on your purposes and the type of data you have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Whitespace tokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer \n",
    "     \n",
    "# Create an instance of WhitespaceTokenizer \n",
    "white_tokenizer = WhitespaceTokenizer() \n",
    "     \n",
    "# Example input\n",
    "sentence = \"This is a \\nsentence with \\n endlines \\n and a tab\\t\\n \"\n",
    "print(sentence)\n",
    "     \n",
    "# Tokenize the sentence\n",
    "sentence = white_tokenizer.tokenize(sentence) \n",
    "     \n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice with WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `RegexpTokenizer()` is another useful `nltk` tokenizer that is used to remove punctuation from text. See how to use it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex Tokenizer Example to remove punctuation\n",
    "# TODO: Run this cell to see output\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize('This is a first-hand experience of nlp!!!  Keep cleaning the data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `nltk.RegexTokenizer()` to remove punctuation from your own sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue Preprocessing data\n",
    "+ Preprocessing text consists of normalizing it with various methods, such as cleaning up punctuation, stop words, stemming etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the stopwords in English that are defined in nltk library\n",
    "# TODO: Run this cell to see the output\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a sentence of your own that includes some of the stopwords in English as defined in the `nltk` lexicon above and then remove them. Output the tokenized version of your sentence stopwords removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
