{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "+ We will be working with Python 3\n",
    "    + We suggest using Jupyter Lab as we will use Python interactively\n",
    "    + [Install Jupyter Notebook](https://jupyter.readthedocs.io/en/latest/install.html) or [Jupyter Lab](https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html) if you don't have anaconda already installed\n",
    "+ Download the Github repository for this tutorial here [https://github.com/ftarlaci/pydata-austin-tutorial.git] \n",
    "    + Browse through the repository and locate the files in the folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Importing Libraries</font>\n",
    "\n",
    "+ Importing the packages we will use today\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Natural Language Toolkit (nltk) is a suite of text processing libraries for classification, \n",
    "    tokenization, stemming, tagging, parsing, and semantic reasoning. \n",
    "    \n",
    "    Installing nltk: \n",
    "    In the command line: \n",
    "    \n",
    "    Windows: pip3 install nltk\n",
    "    Mac: pip3 install -U nltk\n",
    "    Anaconda: conda install -c conda-forge nltk \n",
    "\n",
    "'''\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May be required to ensure the stopwords data is on your machine\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Sentiment Analysis</center>\n",
    "\n",
    "### <font color='blue'>What Is It?</font>\n",
    "\n",
    "+ Sentiment analysis (SA) is **the process of understanding an opinion (sentiment) about a given subject from written or spoken language.** \n",
    "+ It is one of the subfields of Natural Language Processing (NLP) that extracts opinion and attributes from text (or speech). \n",
    "+ Sentiment Analysis is a supervised learning technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLP Tasks Chart](./images/nlpchart.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ SA usually counts on four tasks: \n",
    "    + *opinion identification*, identifying the text which contains an opinion\n",
    "    + *feature extraction*, identifying the aspects being commented on, such as a product's price\n",
    "    + *sentiment classification*, whether the opinion polarity is positive, negative, or neutral\n",
    "    + *visualization and summarization* of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>How Does It Bring Insight to Textual Data?</font>\n",
    "+ It has many practical applications: **product reviews, marketing analysis, customer feedback.**\n",
    "+ It allows us to **make sense of data** in ways that enhance the techniques we use in data science and machine learning for this purpose.\n",
    "+ It is an example of **a classification problem** where a classifier tells if the sentiment in a piece of **text is positive, negative, (or neutral)** or any other class that is defined for classification. \n",
    "+ It allows to gain **insight leading to automate and improve all kinds of processes** by transforming **unstructured data into structured data** of opinions about services, products etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sentiment Smiley and Angry Faces](./images/sa.jpg)\n",
    "\n",
    "*image credit [kdnuggets](https://www.kdnuggets.com/2018/03/5-things-sentiment-analysis-classification.html)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Why Is It Valuable?</font>\n",
    "\n",
    "Organizations deal with enormous amount of data; customer calls, emails, social media posts, among many other variants. **Making these types of data more meaningful and useful requires a lot of effort and time.** One of the core skills in extracting information from and gaining insight into textual data is NLP. **NLP is one of the must have skills for all data scientists** given the constant increase in textual data everywhere we look. \n",
    "\n",
    "+ **Discover how people feel about a particular topic** -- analyze the sentiments of users of various forms (social media, e-commerce, marketing campaigns, among others)\n",
    "+ **Solve a number of business problems** -- product reviews, customer satisfaction...\n",
    "+ **Customer first philosophy** -- any insight into customers through what they like an don't like is exteremly valuable \n",
    "+ **Gateway to data analytics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Basic Terminology</font>\n",
    "\n",
    "+ **Stop Words** -- Removal of words that are not important from the information point of view, such as 'the', 'is' 'a' etc.\n",
    "+ **Tokenization**  -- Segmentation of text into words (a form of feature extraction)\n",
    "+ **Lemmatization** -- Assigning the base forms of words (the lemma of 'spoke' is 'speak' and the lemma of 'languages' is 'language')\n",
    "+ **Stemming** -- Reducing a word to its stem or root form known as a lemma ( car, cars, car’s, cars’ --> car (stem or root word) ) \n",
    "+ **Word Embedding** -- Mapping words to vectors of numbers where words with similar meaning have a similar numerical representation.\n",
    "+ **Text Classification** -- Assigning categories to a document or parts of it\n",
    "+ **N-grams** -- Consideration of a group of words (phrases) rather than single words to extract meaning. Helps with better understanding of text; 'not happy' instead of 'happy,' (e.g bi-gram per token). See the N-Gram Example below:\n",
    "\n",
    "    Sentence:\tThe movie was not great.<br>\n",
    "    Uni-gram\t[‘The’, ‘movie’, ‘was’, ‘not’, ‘great.’]<br>\n",
    "    Bi-grams\t[‘The movie’, ‘was not’, ‘great.’]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <font color='blue'>Open Source NLP Libraries</font>\n",
    "\n",
    "+ Many open source libraries are at your service when you want to implement NLP models\n",
    "\n",
    "    * NLTK\n",
    "    * Spark NLP\n",
    "    * SpaCy\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Text Representation</font>\n",
    "\n",
    "+ Classifiers and learning algorithms cannot directly process the text documents in their original form, as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length\n",
    "+ During the text preprocessing step, the text is converted to a computationally more manageable representation\n",
    "+ **Bag of Words (BoW)** is a common method for extracting features from text:\n",
    "    + In BoW, the presence, count, and often, the frequency of words are taken into consideration. The order in which they occur is ignored.\n",
    "+ Other methods include TF-IDF (term frequency-inverse document frequency), a metric that represents how 'important' a word is to a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--- \n",
    "#### How do we measure the importance of words or the sentiment that originates from them?\n",
    "\n",
    "There are a few methods: \n",
    "\n",
    "1) Term presence and term frequency  \n",
    "  + Term is present \n",
    "  + Term is repeated = weighs more in determining the meaning\n",
    "\n",
    "2) TF-IDF\n",
    "  + TF-IDF stands for Term Frequency and Inverse Document Frequency and in simple terms it means the importance of a term to a document\n",
    "    \n",
    "3) Bag of Words\n",
    "  + extract the words or tokens from the text and then push them in a bag (set) where the words are stored in the bag without any particular order. Thus the presence of a word in the bag is of main importance and the order of the occurrence of the word in the sentence, as well as its grammatical context carries no value. Bag of words scheme is the simplest way of converting text to numbers.  \n",
    "  + --- Loses contextual (positional) data, had disadvantages for other downstream tasks --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>What Will We Do Today?</font>\n",
    "\n",
    "+ Gain a practical understanding of sentiment analysis and its influence on business processes\n",
    "+ Build a sentiment analysis classifier in Python, using some of the open-source NLP and machine learning libraries\n",
    "+ Build a *classification* of sentiments in a customer reviews dataset\n",
    "    + Classification is the process of identifying the category of a new, unseen observation based of a training set of data, which has categories that are known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Workflow:</font>\n",
    "+ Import libraries and inspect the dataset \n",
    "+ Pre-processing data: clean the data using nltk methods\n",
    "+ Create a sentiment classification model\n",
    "+ Evaluate your model with evaluation metrics (Precision, Recall, F1 score, Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue to **`preprocessing_text_with_nltk.ipynb`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
