{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Amazon Alexa Reviews  \n",
    "\n",
    "In this exercise we will explore the sentiments for Amazon Alexa products, such as Alexa Echo, Echo Dot, Firestick etc. This dataset is taken from [Kaggle](https://www.kaggle.com/sid321axn/amazon-alexa-reviews) and consists of 3151 Amazon customer reviews, star ratings, date of review, variant, and feedback on Amazon Alexa products like Alexa Echo, Echo dots, Alexa Firesticks etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import xgboost as xgb   #If we decide to usee XGBoost optimization algorithm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sea\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data \n",
    "Amazon customers reviews for Alexa Echo, Firestick, Echo Dot etc. is the dataset we will use. You can find the dataset here: \n",
    "**./data/amazon_alexa.tsv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if the data file contains the number of reviews we expect.\n",
    "dataset = [line.rstrip() for line in open('../data/amazon_alexa.tsv')]\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the data with Pandas\n",
    "\n",
    "Read the dataset into a Pandas dataframe using pandas.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/amazon_alexa.tsv', delimiter='\\t', quoting=3)\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect some of the data stats. This tells us that the positive feedback, the predominant class in this dataset \n",
    "# is 1, mostly a rating of 5\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can interpret ratings under 3 as negative\n",
    "sea.countplot(\"rating\", hue=\"feedback\", data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting most purchased products\n",
    "plt.figure(figsize=(40,8))\n",
    "sea.countplot(\"variation\", hue=\"feedback\", data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupby('rating').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Text\n",
    "\n",
    "This is where we will apply some of the text preprocessing steps that we have learned in preprocessing notebook and Exercise 1, such as removing stopwords, stemming, removing punctuation from text etc. by using relevant nltk and regex methods to increase the accuracy of our model's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data: extract stopwords, reduce words to their stems, which is a practice that often increases the results of sentiment analysis\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for i in range(0,3150):\n",
    "    \n",
    "    # We are interested in the 'verified_reviews' column of the dataset where we have the text of the review\n",
    "    data = re.sub('[^a-zA-Z]', ' ', dataset['verified_reviews'][i])\n",
    "    data = data.lower()\n",
    "    data = data.split()\n",
    "    \n",
    "    # Create a new Porter stemmer instance, which is one of the stemmers in nltk library to remove morphological affixes from words.\n",
    "    stemmer = PorterStemmer()\n",
    "    # Stem the words that are not one of the stopwords in the English language as defined in nltk\n",
    "    data = [stemmer.stem(word) for word in data if not word in set(stopwords.words('english'))]\n",
    "    data = ' '.join(data)\n",
    "    corpus.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement vectorization of words (feature extraction) as you learned in Exercise 1 with CountVectorizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "# 1500 most occuring words are features for training our classifier.\n",
    "vectorizer = CountVectorizer(max_features=1500)\n",
    "\n",
    "# The function fit_transform() is used for dataset transformations in scikit-learn. \n",
    "# The fit_transform function of the CountVectorizer class converts text documents into corresponding numeric features.\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "print('Displaying word embeddings:\\n ', X)\n",
    "\n",
    "y = dataset.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset into train and test sets. Use sklearn train_test_split() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset to train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This means that X_test and y_test contains 20% of our data which we reserve for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize a classifier from scikit-learn and train it on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sentiment_classifier = RandomForestClassifier(n_estimators = 1000, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit and predict the results of your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   In scikit-learn, an estimator for classification is a Python object that implements the methods fit(X, y) and predict(T)\n",
    "   Train the classifier: Once the model is initialized, we train it to our specific dataset, Scikit-learnâ€™s fit() method \n",
    "   allows us to do so. This is where our machine learning classifier actually learns the underlying functions that produce the results.\n",
    "'''\n",
    "sentiment_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To predict the sentiment for the documents in our test set we can use the predict method of \n",
    "# the RandomForestClassifier class\n",
    "\n",
    "y_pred = sentiment_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a confusion matrix (True/False Negatives/Positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate your model's results using scikit-learn evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congratulations! You have implemented your Sentiment Analysis Model using Natural Language ToolKit and scikit-learn libraries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ For more practice, you can change some other machine learning algorithm to see if you can improve the performance. You can also change the parameters of the CountVectorizer class to see if you can get any improvement or use a different vectorizer, such as `TfidfVectorizer` class from scikit-learn library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
