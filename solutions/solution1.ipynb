{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/Users/fatmatarlaci/Desktop/pydata-tutorial/')  #your desired directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with the provided toy dataset `data/toy_dataset.tsv` and load it into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# get line numbers (optional)\n",
    "dataset = [line.rstrip() for line in open('./data/toy_dataset.tsv')]\n",
    "print(len(dataset))\n",
    "\n",
    "#load into dataframe\n",
    "dataset = pd.read_csv('./data/toy_dataset.tsv', delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus will store your cleaned dataset after preprocessing.\n",
    "corpus = []\n",
    "for i in range(0,26):\n",
    "    data = re.sub('[^a-zA-Z]', ' ', dataset['verified_reviews'][i] )\n",
    "    data = data.lower()\n",
    "    data = data.split()\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    data = [stemmer.stem(word) for word in data if not word in set(stopwords.words('english'))]\n",
    "    data = ' '.join(data)\n",
    "    corpus.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representing Text as Numerical Vectors: Bag of Words\n",
    "+ We first need to represent texts in a way that the learning algorithm can process. \n",
    "+ To represent each word in the dataset, we will convert the text into a matrix of token counts.\n",
    "\n",
    "#### TODO: Inspect the source code below to understand how we represent the text as a matrix for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "vectorizer = CountVectorizer(max_features=1400)\n",
    "\n",
    "# The function fit_transform() is used for dataset transformations in scikit-learn. \n",
    "# Notice that the vectorizer by default stores everything in a sparse array, and using X.toarray()shows us the dense version.\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "y = dataset.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the import `train_test_split()` from `sklearn.model_selection` split our dataset into `train` and `test` sets. Set the 'test_size' to 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This means that X_test and y_test contains 20% of our data which we reserve for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Classifier \n",
    "\n",
    "+ We will use RandomForestClassifier() from scikit-learn library as our classifier. \n",
    "+ Please note, there are a number of classifiers that you can use for Sentiment Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import RandomForestClassifier() from sklearn library and create an instance of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sentiment_classifier = RandomForestClassifier(n_estimators = 100, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit and Predict the test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In scikit-learn, an estimator for classification is a Python object that implements the methods fit(X, y) and predict(T)\n",
    "sentiment_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Here we are predicting the test set results\n",
    "y_pred = sentiment_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue to **exercise2.ipynb**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
