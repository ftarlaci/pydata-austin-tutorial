{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Amazon Alexa Reviews  \n",
    "\n",
    "In this exercise we will explore the sentiments for Amazon Alexa products, such as Alexa Echo, Echo Dot, Firestick etc. This dataset is taken from [Kaggle](https://www.kaggle.com/sid321axn/amazon-alexa-reviews) and consists of 3151 Amazon customer reviews, star ratings, date of review, variant, and feedback on Amazon Alexa products like Alexa Echo, Echo dots, Alexa Firesticks etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "'''\n",
    "VADER (Valence Aware Dictionary and Sentiment Reasoner) is a lexicon and rule-based sentiment analysis \n",
    "tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains.\n",
    "\n",
    "'''\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sea\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the data with Pandas\n",
    "\n",
    "Read the dataset into a Pandas dataframe using `pandas.read_csv()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "dataset = pd.read_csv('../data/amazon_alexa.tsv', delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Inspect the data using some of Pandas methods describe() , info(), groupby() etc. that would give us an overall idea about data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect some of the data stats. This tells us that the positive feedback, the predominant class in this dataset \n",
    "# is 1, mostly a rating of 5\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Run this cell\n",
    "\n",
    "# We can interpret ratings under 3 as negative\n",
    "sea.countplot(\"rating\", hue=\"feedback\", data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Run this cell\n",
    "\n",
    "# Plotting most purchased products \n",
    "plt.figure(figsize=(40,8))\n",
    "sea.countplot(\"variation\", hue=\"feedback\", data=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting data with nltk VADER\n",
    "\n",
    "+ The VADER lexicon in the nltk library calculates negative, positive, and neutral values for our text, and provides a word tokenizer, which splits our data file into sentences or words. Inspect the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Run this cell\n",
    "\n",
    "# Initializing VADER\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Initialize `english.pickle` word tokenizer function \n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Convert our verified_reviews column in the dataframe into a string and tokenize\n",
    "sentences = dataset['verified_reviews']\n",
    "sentences = list(sentences)\n",
    "sentences = str(sentences)\n",
    "sent = tokenizer.tokenize(sentences)\n",
    "\n",
    "\n",
    "'''\n",
    "Next, we will find all sentences in our text file that include a specific keyword and designate \n",
    "these sentences as a list. In this example, we choose \"love\" as our keyword and designate our list \n",
    "of all sentences that include this word \"love_list\".  \n",
    "\n",
    "'''\n",
    "#The \"*\"s are wildcards which match everything before and after the word \"love\" itself.\n",
    "r = re.compile(\".* love .*\")\n",
    "\n",
    "# love_list now contains all the sentences that has the word 'love' in them\n",
    "love_list = list(filter(r.match, sent))\n",
    "\n",
    "print(len(love_list))\n",
    "print(love_list[:10])\n",
    "\n",
    "\n",
    "# We will now run the sentiment analysis on those sentences in the love_list\n",
    "# Running this loop wll allow us to see the calculated positive, negative, \n",
    "# and neutral scores for each sentence that contains the word \"evacuation\" in our text source\n",
    "for sentence in love_list:\n",
    "    print(sentence)\n",
    "    scores = sentiment_analyzer.polarity_scores(sentence)\n",
    "    for key in sorted(scores):\n",
    "        print('{0}: {1}, '.format(key, scores[key]), end='')\n",
    "    print()\n",
    "    \n",
    "# Future practice: change the keyword and inspect the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Text\n",
    "\n",
    "This is where we will apply some of the text preprocessing steps that we have learned in Preprocessing data Notebook, such as removing stopwords, <br> stemming, removing punctuation from text etc. by using relevant nltk and regex methods to get our data ready for training and increase the accuracy of our model's results.\n",
    "\n",
    "+ Lowercase\n",
    "+ Tokenization\n",
    "+ Stemming \n",
    "+ Remove stopwords\n",
    "+ Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Classification\n",
    "\n",
    "Now that we have a clean dataset that we have preprocessed, we are ready to build our sentiment analysis model by using <br> one of the classifiers in the scikit-learn library. For this exercise, we will use a classification model from the scikit-learn library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement vectorization of words (feature extraction) as you learned in Exercise 1 with CountVectorizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset into train and test sets. Use scikit-learn train_test_split() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize a classifier from scikit-learn library and train it on the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can choose another classifier from scikit-learn library if you prefer doing so; \n",
    "# however, it might require some modifications to your code so far. -- Great practice for later!\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit and predict the results of your model using scikit_learn fit() and predict() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a confusion matrix using `scikit-library`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate your results using scikit-learn evaluation metrics\n",
    "\n",
    "+ `scikit-learn` library provides a number of metrics to evaluate the accuracy of models. `accuracy_score()` and `classification_report()` are two of those that you might find useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
